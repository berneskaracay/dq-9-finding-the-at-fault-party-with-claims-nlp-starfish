---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.0'
      jupytext_version: 1.0.5
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
#######################################
print('Setting everything up...')
#######################################

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.lines as mlines
from matplotlib.lines import Line2D

import matplotlib.ticker as ticker
import matplotlib.cm as cm
import matplotlib as mpl

import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import nltk
nltk.download('punkt')
nltk.download('stopwords')
import os
import sys
import mysql.connector
from datetime import datetime
from datetime import date
from datetime import time
from datetime import timedelta
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator



from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix,classification_report
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import SVC
from sklearn import metrics
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import LinearSVC

import seaborn as sns
plt.style.use('ggplot')
pd.options.display.max_rows = 999
pd.options.display.max_columns = 100
pd.options.display.max_colwidth = 100
import spacy


import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
# stop = stopwords.words('english')
nlp = spacy.load('en_core_web_sm')

from IPython.display import HTML as html_print

def cstr(s, color='black'):
    return "<text style=color:{}>{}</text>".format(color, s)


print('done.')

```

```{python}
general_data = pd.read_csv("data/NSS_DS_data.thegeneral.csv")
```

```{python}
general_data.shape
```

```{python}

general_data.describe(include='all')
```

```{python}
general_data.info()
```

```{python}
general_data.isnull().sum(axis = 0)



```

```{python}
general_data.head()
```

```{python}
general_data.nunique()
```

```{python}
general_data.groupby('ClaimGroup').count()
```

```{python}
general_data.groupby('SeverityTypeName').count()
```

```{python}
general_data["AccidentDescription"].head()
```

```{python}

general_data["ClaimLevelBody"][2]
```

```{python}
general_data["ExposureLevelBody"][2]
```

```{python}
general_data['FaultRatingName'].value_counts().plot.bar(title="Freq dist of Fault Rating")
```

```{python}
general_data.AccidentDescription=general_data.AccidentDescription.str.replace('Â½ ', "")

general_data.AccidentDescription=general_data.AccidentDescription.str.replace('*', "")

general_data['AccidentDescription'].str.lower().str.split().head()

stop = stopwords.words('english')
general_data['AccidentDescription'] = general_data['AccidentDescription'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))
general_data.head()

general_data['AccidentDescription'] = general_data['AccidentDescription'].str.replace('[^\w\s]','')


def custom_tokenize(text):
    if not text:
        print('Not text')
        text = ''
    return word_tokenize(text)
general_data['AccidentDescription'] = general_data.AccidentDescription.apply(custom_tokenize)


from collections import defaultdict
for source in general_data:
    word_freq = defaultdict(int)
    for text in general_data.AccidentDescription:
        for word in text:
            word_freq[word] += 1 

df_dict=pd.DataFrame.from_dict(word_freq, orient='index').sort_values(0, ascending=False).reset_index().rename(columns={'index': 'AccidentDescription',0: 'Frequency'})


df_dict.head(1)
```

```{python}
myData=df_dict.head(300)

myData

text=""
for index, row in myData.iterrows():
    str1=""
    str1= (row['AccidentDescription']+" ")*row['Frequency']
    text=text+str1

text[:100]


wordcloud = WordCloud(max_font_size=100, max_words=100, background_color="white", collocations = False).generate(text)
plt.figure(figsize=(40,40))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```

```{python}
# #!pip install wordcloud
```

```{python}
general_data = pd.concat([general_data, pd.get_dummies(general_data['FaultRatingName'])], axis=1)

```

```{python}
general_data.tail()
```

```{python}
def classifier(row):
    if row["FaultRatingName"] == 'Insured at fault':
        return 'Insured at fault'
    elif row["FaultRatingName"] == 'Other party at fault':
        return 'Other party at fault'
    else:
        return 'none'
```

```{python}
    
general_data["label"] = general_data.apply(classifier, axis=1)
```

```{python}
general_data.head()
```

```{python}
general_data['AccidentDescription'][10]
```

```{python}
# Might take awhile...
bows= CountVectorizer(tokenizer=lambda x: x, lowercase=False, ngram_range = (2,4)).fit(general_data['AccidentDescription'])
#bow_transformer= CountVectorizer(ngram_range = (2,3)).fit(df['AccidentDescription'])
```

```{python}
# Print total number of vocab words
print(len(bows.vocabulary_))
```

```{python}
words = general_data['AccidentDescription'][10]
print(words)
```

```{python}
vector_words = bows.transform([words])
print(vector_words )
print(vector_words.shape)
```

```{python}
print(bows.get_feature_names()[46524])
print(bows.get_feature_names()[446746])
```

```{python}
vector_words_info = bows.transform(general_data['AccidentDescription'])
```

```{python}
frequencies = sum(vector_words_info).toarray()[0]
```

```{python}
df_freq =pd.DataFrame(frequencies, index = bows.get_feature_names(), columns = ['frequency'])
df_freq=df_freq.sort_values('frequency', ascending = 'False')
```

```{python}

```

```{python}
df_freq.tail()
```

```{python}
myData=df_freq.tail(300)
```

```{python}
myData
```

```{python}
text=""
for index, row in myData.iterrows():
    str1=""
    str1= (index+",")*row['frequency']
    text=text+str1
```

```{python}
text[:100]
```

```{python}
wordcloud = WordCloud(max_font_size=100, max_words=100, background_color="white").generate(text)
plt.figure(figsize=(40,40))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
plt.show()
```

```{python}

```
